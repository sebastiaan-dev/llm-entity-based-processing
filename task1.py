from llama_cpp import Llama
import spacy
import requests

def get_url(entity):
    url = 'https://wikidata.org/w/api.php'
    params = {
        "action": "wbsearchentities",
        "language": "en",
        "format": "json",
        "search": entity
    }
    #make the request to the wikidata api with the corresponding parameters
    response = requests.get(url, params=params)
    if response.status_code == 200:
        #save the data from the response
        data = response.json()
        #print(data)
        if data["search"]:
            #get the Wikidata entity ID from the search result
            entity_id = data['search'][0]['id']
            
            #entity ID used to get detailed information from Wikidata
            entity_url = f'https://www.wikidata.org/wiki/{entity_id}'
            details_url = f'https://www.wikidata.org/w/api.php'
            
            #get the Wikipedia links
            params = {
                'action': 'wbgetentities',
                'ids': entity_id,
                'sites': 'enwiki',  #the English Wikipedia link
                'props': 'sitelinks',  #we need sitelinks to retrive the original link
                'format': 'json',
            }
            
            response = requests.get(details_url, params=params)
            data = response.json()
            
            #check if the Wikipedia link exists in the response from the json
            if 'entities' in data and entity_id in data['entities']:
                entity_data = data['entities'][entity_id]
                
                if 'sitelinks' in entity_data and 'enwiki' in entity_data['sitelinks']:
                    wikipedia_title = entity_data['sitelinks']['enwiki']['title']
                    wikipedia_link = f"https://en.wikipedia.org/wiki/{wikipedia_title}"
                    return wikipedia_link
            

    return None

#load spacy language model
nlp = spacy.load("en_core_web_sm")
#load the model
model_path = "models/llama-2-7b.Q4_K_M.gguf"

# Prompt the user for input
question = input("Enter your question: ")
#print(question)
#question = "Is Rome the capital of Italy?"
llm = Llama(model_path=model_path, verbose=False)
#save the response from the llm to our question
response = llm(question)

# Extract the generated text from the response
answer = response["choices"][0]["text"]
#process the text generated by llama for the entity recognition
doc = nlp(answer)
#dictionary for saving the entities with the corresponding url
entity_url = {}
for ent in doc.ents:
    #scrape the entities by retrieving only the important ones
    if ent.label_ in ["GPE", "ORG", "LOC", "PERSON", "EVENT", "WORK_OF_ART", "NORP", "FAC", "LAW"]:
        url = get_url(ent.text)
        if url:
            entity_url[ent.text] = url

#print all the entities with the corresponding links
for entity, url in entity_url.items():
    print(f"{entity} -> {url}")
